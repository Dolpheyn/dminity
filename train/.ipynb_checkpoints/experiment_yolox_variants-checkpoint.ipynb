{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfVlxlYYBR6z"
   },
   "source": [
    "# Install YOLOX Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "owSOGJ7qHoDm",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0406ab58-2e91-4a72-e860-92f2c3dcc2e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n",
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd /notebooks\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "igwruhYxE_a7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "92ca43e7-7d76-4345-d2cc-9c92c0d8d498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/YOLOX\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (21.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.21.2)\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.7.1+cu110)\n",
      "Requirement already satisfied: opencv_python in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (4.5.4.60)\n",
      "Requirement already satisfied: loguru in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (0.5.3)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (0.18.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (4.62.3)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.8.2+cu110)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (8.4.0)\n",
      "Requirement already satisfied: thop in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (0.0.31.post2005241907)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.10.2.3)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (0.8.9)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (2.6.0)\n",
      "Requirement already satisfied: onnx==1.8.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (1.8.1)\n",
      "Requirement already satisfied: onnxruntime==1.8.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (1.8.0)\n",
      "Requirement already satisfied: onnx-simplifier==0.3.5 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (0.3.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.8/site-packages (from onnx==1.8.1->-r requirements.txt (line 16)) (3.10.0.2)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.8/site-packages (from onnx==1.8.1->-r requirements.txt (line 16)) (3.18.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from onnx==1.8.1->-r requirements.txt (line 16)) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.8/site-packages (from onnxruntime==1.8.0->-r requirements.txt (line 17)) (2.0)\n",
      "Requirement already satisfied: onnxoptimizer>=0.2.5 in /opt/conda/lib/python3.8/site-packages (from onnx-simplifier==0.3.5->-r requirements.txt (line 18)) (0.2.6)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 6)) (2.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 6)) (2.13.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 6)) (2021.11.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 6)) (3.4.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (1.41.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (1.35.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (2.0.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (2.26.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (0.14.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 13)) (58.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 13)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 13)) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 13)) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 6)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 6)) (0.10.0)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /opt/conda/lib/python3.8/site-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 6)) (5.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 13)) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 13)) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 13)) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 13)) (2021.5.30)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 13)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 13)) (3.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Using pip 21.3.1 from /opt/conda/lib/python3.8/site-packages/pip (python 3.8)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///notebooks/YOLOX\n",
      "  Running command python setup.py egg_info\n",
      "  running egg_info\n",
      "  creating /tmp/pip-pip-egg-info-5p_u3az_/yolox.egg-info\n",
      "  writing /tmp/pip-pip-egg-info-5p_u3az_/yolox.egg-info/PKG-INFO\n",
      "  writing dependency_links to /tmp/pip-pip-egg-info-5p_u3az_/yolox.egg-info/dependency_links.txt\n",
      "  writing top-level names to /tmp/pip-pip-egg-info-5p_u3az_/yolox.egg-info/top_level.txt\n",
      "  writing manifest file '/tmp/pip-pip-egg-info-5p_u3az_/yolox.egg-info/SOURCES.txt'\n",
      "  reading manifest file '/tmp/pip-pip-egg-info-5p_u3az_/yolox.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file '/tmp/pip-pip-egg-info-5p_u3az_/yolox.egg-info/SOURCES.txt'\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Installing collected packages: yolox\n",
      "  Attempting uninstall: yolox\n",
      "    Found existing installation: yolox 0.1.0\n",
      "    Uninstalling yolox-0.1.0:\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/yolox.egg-link\n",
      "      Removing pth entries from /opt/conda/lib/python3.8/site-packages/easy-install.pth:\n",
      "      Removing entry: /notebooks/YOLOX\n",
      "      Successfully uninstalled yolox-0.1.0\n",
      "  Running setup.py develop for yolox\n",
      "    Running command /opt/conda/bin/python3.8 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/notebooks/YOLOX/setup.py'\"'\"'; __file__='\"'\"'/notebooks/YOLOX/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps\n",
      "    running develop\n",
      "    running egg_info\n",
      "    writing yolox.egg-info/PKG-INFO\n",
      "    writing dependency_links to yolox.egg-info/dependency_links.txt\n",
      "    writing top-level names to yolox.egg-info/top_level.txt\n",
      "    reading manifest file 'yolox.egg-info/SOURCES.txt'\n",
      "    adding license file 'LICENSE'\n",
      "    writing manifest file 'yolox.egg-info/SOURCES.txt'\n",
      "    running build_ext\n",
      "    building 'yolox._C' extension\n",
      "    Emitting ninja build file /notebooks/YOLOX/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "    Compiling objects...\n",
      "    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "    [1/2] c++ -MMD -MF /notebooks/YOLOX/build/temp.linux-x86_64-3.8/notebooks/YOLOX/yolox/layers/csrc/cocoeval/cocoeval.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/notebooks/YOLOX/yolox/layers/csrc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/include/python3.8 -c -c /notebooks/YOLOX/yolox/layers/csrc/cocoeval/cocoeval.cpp -o /notebooks/YOLOX/build/temp.linux-x86_64-3.8/notebooks/YOLOX/yolox/layers/csrc/cocoeval/cocoeval.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    [2/2] c++ -MMD -MF /notebooks/YOLOX/build/temp.linux-x86_64-3.8/notebooks/YOLOX/yolox/layers/csrc/vision.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/notebooks/YOLOX/yolox/layers/csrc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/include/python3.8 -c -c /notebooks/YOLOX/yolox/layers/csrc/vision.cpp -o /notebooks/YOLOX/build/temp.linux-x86_64-3.8/notebooks/YOLOX/yolox/layers/csrc/vision.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /notebooks/YOLOX/build/temp.linux-x86_64-3.8/notebooks/YOLOX/yolox/layers/csrc/vision.o /notebooks/YOLOX/build/temp.linux-x86_64-3.8/notebooks/YOLOX/yolox/layers/csrc/cocoeval/cocoeval.o -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/yolox/_C.cpython-38-x86_64-linux-gnu.so\n",
      "    copying build/lib.linux-x86_64-3.8/yolox/_C.cpython-38-x86_64-linux-gnu.so -> yolox\n",
      "    Creating /opt/conda/lib/python3.8/site-packages/yolox.egg-link (link to .)\n",
      "    Adding yolox 0.1.0 to easy-install.pth file\n",
      "\n",
      "    Installed /notebooks/YOLOX\n",
      "Successfully installed yolox-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/dolpheyn/YOLOX.git\n",
    "%cd /notebooks/YOLOX\n",
    "\n",
    "!pip3 install -U pip && pip3 install -r requirements.txt\n",
    "!pip3 install -v -e .  \n",
    "\n",
    "#!pip uninstall -y torch torchvision torchaudio\n",
    "# May need to change in the future if Colab no longer uses CUDA 11.0\n",
    "#!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install torch==1.10.0+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llsu3xhVBZYC"
   },
   "source": [
    "## Install Nvidia Apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ksHd57LFFMzK",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "094e8d21-c6f4-4ea2-e054-8d396b850f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n",
      "/notebooks/apex\n",
      "/opt/conda/lib/python3.8/site-packages/pip/_internal/commands/install.py:245: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
      "  cmdoptions.check_install_build_global(options)\n",
      "Using pip 21.3.1 from /opt/conda/lib/python3.8/site-packages/pip (python 3.8)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Processing /notebooks/apex\n",
      "  Running command python setup.py egg_info\n",
      "\n",
      "\n",
      "  torch.__version__  = 1.7.1+cu110\n",
      "\n",
      "\n",
      "  running egg_info\n",
      "  creating /tmp/pip-pip-egg-info-261k4813/apex.egg-info\n",
      "  writing /tmp/pip-pip-egg-info-261k4813/apex.egg-info/PKG-INFO\n",
      "  writing dependency_links to /tmp/pip-pip-egg-info-261k4813/apex.egg-info/dependency_links.txt\n",
      "  writing top-level names to /tmp/pip-pip-egg-info-261k4813/apex.egg-info/top_level.txt\n",
      "  writing manifest file '/tmp/pip-pip-egg-info-261k4813/apex.egg-info/SOURCES.txt'\n",
      "  reading manifest file '/tmp/pip-pip-egg-info-261k4813/apex.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file '/tmp/pip-pip-egg-info-261k4813/apex.egg-info/SOURCES.txt'\n",
      "  /notebooks/apex/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
      "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Skipping wheel build for apex, due to binaries being disabled for it.\n",
      "Installing collected packages: apex\n",
      "  Attempting uninstall: apex\n",
      "    Found existing installation: apex 0.1\n",
      "    Uninstalling apex-0.1:\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/amp_C.cpython-38-x86_64-linux-gnu.so\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/apex-0.1-py3.8.egg-info\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/apex/\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/apex_C.cpython-38-x86_64-linux-gnu.so\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
      "      Removing file or directory /opt/conda/lib/python3.8/site-packages/syncbn.cpython-38-x86_64-linux-gnu.so\n",
      "      Successfully uninstalled apex-0.1\n",
      "    Running command /opt/conda/bin/python3.8 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/notebooks/apex/setup.py'\"'\"'; __file__='\"'\"'/notebooks/apex/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-7ong8nue/install-record.txt --single-version-externally-managed --compile --install-headers /opt/conda/include/python3.8/apex\n",
      "\n",
      "\n",
      "    torch.__version__  = 1.7.1+cu110\n",
      "\n",
      "\n",
      "    /notebooks/apex/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
      "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-3.8\n",
      "    creating build/lib.linux-x86_64-3.8/apex\n",
      "    copying apex/__init__.py -> build/lib.linux-x86_64-3.8/apex\n",
      "    copying apex/_autocast_utils.py -> build/lib.linux-x86_64-3.8/apex\n",
      "    creating build/lib.linux-x86_64-3.8/apex/normalization\n",
      "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
      "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
      "    creating build/lib.linux-x86_64-3.8/apex/transformer\n",
      "    copying apex/transformer/log_util.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
      "    copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
      "    copying apex/transformer/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
      "    copying apex/transformer/enums.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
      "    copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
      "    copying apex/transformer/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
      "    creating build/lib.linux-x86_64-3.8/apex/fused_dense\n",
      "    copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-3.8/apex/fused_dense\n",
      "    copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-3.8/apex/fused_dense\n",
      "    creating build/lib.linux-x86_64-3.8/apex/reparameterization\n",
      "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.8/apex/reparameterization\n",
      "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.8/apex/reparameterization\n",
      "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.8/apex/reparameterization\n",
      "    creating build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
      "    creating build/lib.linux-x86_64-3.8/apex/mlp\n",
      "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.8/apex/mlp\n",
      "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.8/apex/mlp\n",
      "    creating build/lib.linux-x86_64-3.8/apex/parallel\n",
      "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
      "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
      "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
      "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
      "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
      "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
      "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
      "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
      "    creating build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
      "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
      "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
      "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
      "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
      "    creating build/lib.linux-x86_64-3.8/apex/optimizers\n",
      "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
      "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
      "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
      "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
      "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
      "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
      "    creating build/lib.linux-x86_64-3.8/apex/RNN\n",
      "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
      "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
      "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
      "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
      "    creating build/lib.linux-x86_64-3.8/apex/pyprof\n",
      "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.8/apex/pyprof\n",
      "    creating build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
      "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
      "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
      "    creating build/lib.linux-x86_64-3.8/apex/contrib\n",
      "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib\n",
      "    creating build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
      "    copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
      "    copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
      "    creating build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
      "    copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
      "    copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
      "    copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
      "    copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
      "    creating build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
      "    copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
      "    copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
      "    creating build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
      "    copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
      "    copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
      "    copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
      "    copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
      "    copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
      "    copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
      "    creating build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
      "    copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
      "    copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
      "    creating build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
      "    copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
      "    copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
      "    copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
      "    copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
      "    copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
      "    copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
      "    copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
      "    copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
      "    creating build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
      "    copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
      "    copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
      "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
      "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
      "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
      "    creating build/lib.linux-x86_64-3.8/apex/amp/lists\n",
      "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
      "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
      "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
      "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
      "    creating build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
      "    creating build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
      "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
      "    creating build/lib.linux-x86_64-3.8/apex/pyprof/nvtx\n",
      "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.8/apex/pyprof/nvtx\n",
      "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.8/apex/pyprof/nvtx\n",
      "    creating build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
      "    copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
      "    copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
      "    copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
      "    copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
      "    creating build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
      "    creating build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
      "    copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
      "    copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
      "    creating build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
      "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
      "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
      "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
      "    creating build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
      "    copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
      "    copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
      "    creating build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
      "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
      "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
      "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
      "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
      "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
      "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
      "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
      "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
      "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
      "    creating build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
      "    copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
      "    copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
      "    creating build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
      "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
      "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
      "    creating build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
      "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
      "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
      "    running build_ext\n",
      "    building 'apex_C' extension\n",
      "    creating /notebooks/apex/build/temp.linux-x86_64-3.8\n",
      "    creating /notebooks/apex/build/temp.linux-x86_64-3.8/csrc\n",
      "    Emitting ninja build file /notebooks/apex/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "    Compiling objects...\n",
      "    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "    [1/1] c++ -MMD -MF /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/flatten_unflatten.cpp -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:149,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/flatten_unflatten.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ParallelOpenMP.h:84: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
      "       84 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
      "          |\n",
      "    In file included from /notebooks/apex/csrc/flatten_unflatten.cpp:2:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       36 |     return tensors[0].type();\n",
      "          |                            ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/flatten_unflatten.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/apex_C.cpython-38-x86_64-linux-gnu.so\n",
      "    building 'amp_C' extension\n",
      "    Emitting ninja build file /notebooks/apex/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "    Compiling objects...\n",
      "    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "    [1/12] c++ -MMD -MF /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/amp_C_frontend.cpp -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:149,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/amp_C_frontend.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ParallelOpenMP.h:84: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
      "       84 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
      "          |\n",
      "    [2/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_lamb_stage_2.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    [3/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_scale_kernel.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    [4/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_adam.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    [5/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_adagrad.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    [6/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_l2norm_kernel.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    [7/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_l2norm_scale_kernel.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    [8/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_sgd_kernel.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    [9/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_axpby_kernel.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    [10/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_lamb_stage_1.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    [11/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_novograd.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    [12/12] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/multi_tensor_lamb.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_sgd_kernel.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_scale_kernel.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_axpby_kernel.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_scale_kernel.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_1.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_2.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_adam.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_adagrad.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_novograd.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb.o -L/opt/conda/lib/python3.8/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.8/amp_C.cpython-38-x86_64-linux-gnu.so\n",
      "    building 'syncbn' extension\n",
      "    Emitting ninja build file /notebooks/apex/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "    Compiling objects...\n",
      "    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "    [1/2] c++ -MMD -MF /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/syncbn.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/syncbn.cpp -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:149,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/syncbn.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ParallelOpenMP.h:84: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
      "       84 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
      "          |\n",
      "    [2/2] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/welford.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/syncbn.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/welford.o -L/opt/conda/lib/python3.8/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.8/syncbn.cpython-38-x86_64-linux-gnu.so\n",
      "    building 'fused_layer_norm_cuda' extension\n",
      "    Emitting ninja build file /notebooks/apex/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "    Compiling objects...\n",
      "    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "    [1/2] c++ -MMD -MF /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/layer_norm_cuda.cpp -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:149,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ParallelOpenMP.h:84: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
      "       84 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
      "          |\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      129 |   CHECK_INPUT(input);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      150 |   CHECK_INPUT(input);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      151 |   CHECK_INPUT(gamma);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:152:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      152 |   CHECK_INPUT(beta);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine_mixed_dtypes(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:174:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      174 |   CHECK_INPUT(input);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:216:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      216 |   CHECK_INPUT(dout);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:217:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      217 |   CHECK_INPUT(mean);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      218 |   CHECK_INPUT(invvar);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      219 |   CHECK_INPUT(input);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:242:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      242 |   CHECK_INPUT(dout);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:243:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      243 |   CHECK_INPUT(mean);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:244:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      244 |   CHECK_INPUT(invvar);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:245:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      245 |   CHECK_INPUT(input);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:246:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      246 |   CHECK_INPUT(gamma);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/c10/core/Allocator.h:6,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:7,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                                          ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
      "      171 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
      "          |                                                                 ^~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
      "      330 |   if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
      "          |       ^~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
      "      318 |   TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
      "      341 | #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
      "          |                                ^~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
      "      117 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
      "      119 | #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
      "          |                        ^~~~~~~~~~\n",
      "    /notebooks/apex/csrc/layer_norm_cuda.cpp:247:3: note: in expansion of macro ‘CHECK_INPUT’\n",
      "      247 |   CHECK_INPUT(beta);\n",
      "          |   ^~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/layer_norm_cuda.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    [2/2] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/layer_norm_cuda_kernel.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda_kernel.o -L/opt/conda/lib/python3.8/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.8/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so\n",
      "    building 'mlp_cuda' extension\n",
      "    Emitting ninja build file /notebooks/apex/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "    Compiling objects...\n",
      "    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "    [1/2] c++ -MMD -MF /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/mlp.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/mlp.cpp -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:149,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ParallelOpenMP.h:84: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
      "       84 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
      "          |\n",
      "    /notebooks/apex/csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
      "    /notebooks/apex/csrc/mlp.cpp:57:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "       57 |   for (int i = 0; i < num_layers; i++) {\n",
      "          |                   ~~^~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       64 |   auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
      "          |                                                                             ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       65 |   auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
      "          |                                                                   ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:65:36: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ [-Wnarrowing]\n",
      "       65 |   auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
      "          |                                    ^~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:65:36: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ [-Wnarrowing]\n",
      "    /notebooks/apex/csrc/mlp.cpp:67:59: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       67 |   auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
      "          |                                                           ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /notebooks/apex/csrc/mlp.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/mlp.cpp:69:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "          |                                                      ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      150 |     const auto& the_type = TYPE;                                            \\\n",
      "          |                            ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "       72 |     for (int i = 0; i < num_layers; i++) {\n",
      "          |                     ~~^~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "       78 |     auto result = mlp_fp<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "       72 |     for (int i = 0; i < num_layers; i++) {\n",
      "          |                     ~~^~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "       78 |     auto result = mlp_fp<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "       72 |     for (int i = 0; i < num_layers; i++) {\n",
      "          |                     ~~^~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "       78 |     auto result = mlp_fp<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
      "    /notebooks/apex/csrc/mlp.cpp:115:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "      115 |   for (int i = 0; i < num_layers; i++) {\n",
      "          |                   ~~^~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:120:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
      "      120 |   for (int i = 0; i < inputs.size(); i++) {\n",
      "          |                   ~~^~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:121:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      121 |     outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
      "          |                                                                   ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /notebooks/apex/csrc/mlp.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |                                                      ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      150 |     const auto& the_type = TYPE;                                            \\\n",
      "          |                            ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "      126 |     for (int i = 0; i < num_layers; i++) {\n",
      "          |                     ~~^~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
      "      130 |     for (int i = 0; i < inputs.size(); i++) {\n",
      "          |                     ~~^~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:138:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      138 |     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
      "          |                                                                                ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /notebooks/apex/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ [-Wnarrowing]\n",
      "      138 |     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
      "          |                                  ~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ [-Wnarrowing]\n",
      "      138 |     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
      "          |                                  ~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "      140 |     auto result = mlp_bp<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "      126 |     for (int i = 0; i < num_layers; i++) {\n",
      "          |                     ~~^~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
      "      130 |     for (int i = 0; i < inputs.size(); i++) {\n",
      "          |                     ~~^~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:138:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      138 |     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
      "          |                                                                                ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /notebooks/apex/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ [-Wnarrowing]\n",
      "      138 |     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
      "          |                                  ~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ [-Wnarrowing]\n",
      "      138 |     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
      "          |                                  ~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "      140 |     auto result = mlp_bp<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "      126 |     for (int i = 0; i < num_layers; i++) {\n",
      "          |                     ~~^~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
      "      130 |     for (int i = 0; i < inputs.size(); i++) {\n",
      "          |                     ~~^~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:138:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      138 |     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
      "          |                                                                                ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/mlp.cpp:1:\n",
      "    /notebooks/apex/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ [-Wnarrowing]\n",
      "      138 |     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
      "          |                                  ~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ [-Wnarrowing]\n",
      "      138 |     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
      "          |                                  ~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "      140 |     auto result = mlp_bp<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    [2/2] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/mlp_cuda.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/mlp.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/mlp_cuda.o -L/opt/conda/lib/python3.8/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.8/mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
      "    building 'fused_dense_cuda' extension\n",
      "    Emitting ninja build file /notebooks/apex/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "    Compiling objects...\n",
      "    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "    [1/2] c++ -MMD -MF /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/fused_dense.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/fused_dense.cpp -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:149,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ParallelOpenMP.h:84: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
      "       84 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
      "          |\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In function ‘at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)’:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:30:63: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       30 |   auto out = at::empty({batch_size, out_features}, input.type());\n",
      "          |                                                               ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:33:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       33 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
      "          |                                                       ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:35:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       35 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
      "          |                                                  ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      150 |     const auto& the_type = TYPE;                                            \\\n",
      "          |                            ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       35 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       35 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
      "       37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
      "          |               ^~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       35 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "       38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       35 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
      "       37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
      "          |               ^~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       35 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "       38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       35 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
      "       37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
      "          |               ^~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       35 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "       38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       35 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)’:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:64:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       64 |   auto d_weight = at::empty({out_features, in_features}, input.type());\n",
      "          |                                                                     ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:68:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       68 |   auto d_bias = at::empty({out_features}, input.type());\n",
      "          |                                                      ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:70:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       70 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
      "          |                                                                  ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:73:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       73 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
      "          |                                                       ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:75:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "       75 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |                                                  ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      150 |     const auto& the_type = TYPE;                                            \\\n",
      "          |                            ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       75 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       75 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
      "       77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
      "          |               ^~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       75 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "       78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       75 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
      "       77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
      "          |               ^~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       75 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "       78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       75 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
      "       77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
      "          |               ^~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       75 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "       78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "       75 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:106:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      106 |   auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
      "          |                                                                      ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:107:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      107 |   auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
      "          |                                                                      ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:108:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      108 |   auto output2 = at::empty({batch_size, out_features}, input.type());\n",
      "          |                                                                   ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:111:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      111 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
      "          |                                                       ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:113:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      113 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
      "          |                                                  ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      150 |     const auto& the_type = TYPE;                                            \\\n",
      "          |                            ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      113 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      113 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "      118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      113 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "      118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      113 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "      118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      113 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:149:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      149 |   auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
      "          |                                                                         ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:150:74: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      150 |   auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
      "          |                                                                          ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:151:58: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      151 |   auto d_bias1 = at::empty({hidden_features}, input.type());\n",
      "          |                                                          ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:152:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      152 |   auto d_bias2 = at::empty({out_features}, input.type());\n",
      "          |                                                       ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:153:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      153 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
      "          |                                                                  ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:154:72: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      154 |   auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
      "          |                                                                        ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:157:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      157 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
      "          |                                                       ^\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:159:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "      159 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |                                                  ^\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      150 |     const auto& the_type = TYPE;                                            \\\n",
      "          |                            ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Context.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:9,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
      "      277 |   DeprecatedTypeProperties & type() const {\n",
      "          |                              ^~~~\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ATen.h:13,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/fused_dense.cpp:1:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      159 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "      152 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "          |                                                        ^\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      159 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
      "       66 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "          |                       ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "      162 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      159 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "      162 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      159 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "      162 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
      "          |          ^~~~~~\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
      "       13 |     return __VA_ARGS__();                          \\\n",
      "          |            ^~~~~~~~~~~\n",
      "    /notebooks/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "      159 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
      "          |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    [2/2] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/fused_dense_cuda.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/fused_dense.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/fused_dense_cuda.o -L/opt/conda/lib/python3.8/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.8/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so\n",
      "    building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
      "    creating /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron\n",
      "    Emitting ninja build file /notebooks/apex/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "    Compiling objects...\n",
      "    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "    [1/2] c++ -MMD -MF /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/notebooks/apex/csrc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:149,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/megatron/scaled_upper_triang_masked_softmax.cpp:18:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ParallelOpenMP.h:84: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
      "       84 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
      "          |\n",
      "    [2/2] /usr/local/cuda/bin/nvcc -I/notebooks/apex/csrc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/opt/conda/lib/python3.8/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.8/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
      "    building 'scaled_masked_softmax_cuda' extension\n",
      "    Emitting ninja build file /notebooks/apex/build/temp.linux-x86_64-3.8/build.ninja...\n",
      "    Compiling objects...\n",
      "    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "    [1/2] c++ -MMD -MF /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/notebooks/apex/csrc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/megatron/scaled_masked_softmax.cpp -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "    In file included from /opt/conda/lib/python3.8/site-packages/torch/include/ATen/Parallel.h:149,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
      "                     from /opt/conda/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                     from /notebooks/apex/csrc/megatron/scaled_masked_softmax.cpp:18:\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/ParallelOpenMP.h:84: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
      "       84 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
      "          |\n",
      "    [2/2] /usr/local/cuda/bin/nvcc -I/notebooks/apex/csrc -I/opt/conda/lib/python3.8/site-packages/torch/include -I/opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.8/site-packages/torch/include/TH -I/opt/conda/lib/python3.8/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.8 -c -c /notebooks/apex/csrc/megatron/scaled_masked_softmax_cuda.cu -o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    /opt/conda/lib/python3.8/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o /notebooks/apex/build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax_cuda.o -L/opt/conda/lib/python3.8/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.8/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
      "    running install_lib\n",
      "    copying build/lib.linux-x86_64-3.8/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /opt/conda/lib/python3.8/site-packages\n",
      "    copying build/lib.linux-x86_64-3.8/apex_C.cpython-38-x86_64-linux-gnu.so -> /opt/conda/lib/python3.8/site-packages\n",
      "    copying build/lib.linux-x86_64-3.8/syncbn.cpython-38-x86_64-linux-gnu.so -> /opt/conda/lib/python3.8/site-packages\n",
      "    copying build/lib.linux-x86_64-3.8/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so -> /opt/conda/lib/python3.8/site-packages\n",
      "    copying build/lib.linux-x86_64-3.8/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /opt/conda/lib/python3.8/site-packages\n",
      "    copying build/lib.linux-x86_64-3.8/mlp_cuda.cpython-38-x86_64-linux-gnu.so -> /opt/conda/lib/python3.8/site-packages\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/normalization\n",
      "    copying build/lib.linux-x86_64-3.8/apex/normalization/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/normalization\n",
      "    copying build/lib.linux-x86_64-3.8/apex/normalization/fused_layer_norm.py -> /opt/conda/lib/python3.8/site-packages/apex/normalization\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/transformer\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/log_util.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/transformer/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/amp/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/amp/grad_scaler.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/parallel_state.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/p2p_communication.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/common.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/_timers.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/utils.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/enums.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/transformer/_data\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/_data/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/_data\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/_data/_batchsampler.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/_data\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/transformer/testing\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_gpt.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/testing\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/testing\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/global_vars.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/testing\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_bert.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/testing\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/commons.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/testing\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/arguments.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/testing\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/microbatches.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/utils.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/transformer/functional\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/functional/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/functional\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/functional/fused_softmax.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/functional\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/memory.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/data.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/utils.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/random.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/cross_entropy.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/mappings.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/layers.py -> /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/fused_dense\n",
      "    copying build/lib.linux-x86_64-3.8/apex/fused_dense/fused_dense.py -> /opt/conda/lib/python3.8/site-packages/apex/fused_dense\n",
      "    copying build/lib.linux-x86_64-3.8/apex/fused_dense/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/fused_dense\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.8/apex/reparameterization/reparameterization.py -> /opt/conda/lib/python3.8/site-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.8/apex/reparameterization/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.8/apex/reparameterization/weight_norm.py -> /opt/conda/lib/python3.8/site-packages/apex/reparameterization\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/_process_optimizer.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/handle.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/torch_overrides.py -> /opt/conda/lib/python3.8/site-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/functional_overrides.py -> /opt/conda/lib/python3.8/site-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/tensor_overrides.py -> /opt/conda/lib/python3.8/site-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/compat.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/rnn_compat.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/frontend.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/scaler.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/utils.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/opt.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/wrap.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/__version__.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/amp.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/_amp_state.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/amp/_initialize.py -> /opt/conda/lib/python3.8/site-packages/apex/amp\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/mlp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/mlp/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/mlp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/mlp/mlp.py -> /opt/conda/lib/python3.8/site-packages/apex/mlp\n",
      "    copying build/lib.linux-x86_64-3.8/apex/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/parallel/distributed.py -> /opt/conda/lib/python3.8/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/parallel/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/parallel/sync_batchnorm_kernel.py -> /opt/conda/lib/python3.8/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/parallel/optimized_sync_batchnorm.py -> /opt/conda/lib/python3.8/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/parallel/multiproc.py -> /opt/conda/lib/python3.8/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/parallel/optimized_sync_batchnorm_kernel.py -> /opt/conda/lib/python3.8/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/parallel/sync_batchnorm.py -> /opt/conda/lib/python3.8/site-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.8/apex/parallel/LARC.py -> /opt/conda/lib/python3.8/site-packages/apex/parallel\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/fp16util.py -> /opt/conda/lib/python3.8/site-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/fp16_optimizer.py -> /opt/conda/lib/python3.8/site-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/loss_scaler.py -> /opt/conda/lib/python3.8/site-packages/apex/fp16_utils\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_adam.py -> /opt/conda/lib/python3.8/site-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/optimizers/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_sgd.py -> /opt/conda/lib/python3.8/site-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_novograd.py -> /opt/conda/lib/python3.8/site-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_adagrad.py -> /opt/conda/lib/python3.8/site-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_lamb.py -> /opt/conda/lib/python3.8/site-packages/apex/optimizers\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.8/apex/RNN/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.8/apex/RNN/cells.py -> /opt/conda/lib/python3.8/site-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.8/apex/RNN/RNNBackend.py -> /opt/conda/lib/python3.8/site-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.8/apex/RNN/models.py -> /opt/conda/lib/python3.8/site-packages/apex/RNN\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/pyprof\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/blas.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/activation.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/pointwise.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/conv.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/usage.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/prof.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/dropout.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/linear.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/optim.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/output.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/normalization.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/recurrentCell.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/data.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/softmax.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/misc.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/index_slice_join_mutate.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/embedding.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/randomSample.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/base.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/utility.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/__main__.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/convert.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/loss.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/pooling.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/reduction.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/nvvp.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/db.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/kernel.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/__main__.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/parse.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/pyprof/nvtx\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/nvtx/nvmarker.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/nvtx\n",
      "    copying build/lib.linux-x86_64-3.8/apex/pyprof/nvtx/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/pyprof/nvtx\n",
      "    copying build/lib.linux-x86_64-3.8/apex/_autocast_utils.py -> /opt/conda/lib/python3.8/site-packages/apex\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/multi_tensor_apply\n",
      "    copying build/lib.linux-x86_64-3.8/apex/multi_tensor_apply/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/multi_tensor_apply\n",
      "    copying build/lib.linux-x86_64-3.8/apex/multi_tensor_apply/multi_tensor_apply.py -> /opt/conda/lib/python3.8/site-packages/apex/multi_tensor_apply\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/contrib\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/contrib/bottleneck\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/bottleneck.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/bottleneck\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/bottleneck_module_test.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/bottleneck\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/bottleneck\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/test.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/bottleneck\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/self_multihead_attn.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/contrib/layer_norm\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/layer_norm/layer_norm.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/layer_norm\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/layer_norm/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/layer_norm\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/contrib/sparsity\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/asp.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/sparsity\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/sparsity\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/sparse_masklib.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/sparsity\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/contrib/transducer\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/transducer.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/transducer\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/transducer\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_adam.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fp16_optimizer.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_sgd.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_lamb.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_adam.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_lamb.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/contrib/fmha\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/fmha/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/fmha\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/fmha/fmha.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/fmha\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/contrib/groupbn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/groupbn/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/groupbn\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/groupbn/batch_norm.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/groupbn\n",
      "    creating /opt/conda/lib/python3.8/site-packages/apex/contrib/xentropy\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/xentropy/softmax_xentropy.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/xentropy\n",
      "    copying build/lib.linux-x86_64-3.8/apex/contrib/xentropy/__init__.py -> /opt/conda/lib/python3.8/site-packages/apex/contrib/xentropy\n",
      "    copying build/lib.linux-x86_64-3.8/amp_C.cpython-38-x86_64-linux-gnu.so -> /opt/conda/lib/python3.8/site-packages\n",
      "    copying build/lib.linux-x86_64-3.8/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so -> /opt/conda/lib/python3.8/site-packages\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/normalization/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/log_util.py to log_util.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/amp/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/amp/grad_scaler.py to grad_scaler.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/parallel_state.py to parallel_state.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/pipeline_parallel/utils.py to utils.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/enums.py to enums.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/_data/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/_data/_batchsampler.py to _batchsampler.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/testing/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/testing/global_vars.py to global_vars.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/testing/standalone_bert.py to standalone_bert.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/testing/commons.py to commons.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/testing/arguments.py to arguments.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/microbatches.py to microbatches.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/utils.py to utils.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/functional/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel/memory.py to memory.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel/data.py to data.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel/utils.py to utils.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel/random.py to random.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/transformer/tensor_parallel/layers.py to layers.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/fused_dense/fused_dense.py to fused_dense.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/fused_dense/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/reparameterization/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/handle.py to handle.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/lists/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/compat.py to compat.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/frontend.py to frontend.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/scaler.py to scaler.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/utils.py to utils.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/opt.py to opt.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/wrap.py to wrap.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/__version__.py to __version__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/amp.py to amp.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/_amp_state.py to _amp_state.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/amp/_initialize.py to _initialize.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/mlp/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/mlp/mlp.py to mlp.cpython-38.pyc\n",
      "    /opt/conda/lib/python3.8/site-packages/apex/mlp/mlp.py:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "      if activation is 'none':\n",
      "    /opt/conda/lib/python3.8/site-packages/apex/mlp/mlp.py:42: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "      elif activation is 'relu':\n",
      "    /opt/conda/lib/python3.8/site-packages/apex/mlp/mlp.py:44: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "      elif activation is 'sigmoid':\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py to distributed.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/parallel/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/parallel/multiproc.py to multiproc.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/parallel/LARC.py to LARC.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/fp16_utils/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/optimizers/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/RNN/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/RNN/cells.py to cells.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/RNN/models.py to models.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/blas.py to blas.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/activation.py to activation.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/conv.py to conv.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/usage.py to usage.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/prof.py to prof.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/dropout.py to dropout.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/linear.py to linear.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/optim.py to optim.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/output.py to output.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/normalization.py to normalization.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/data.py to data.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/softmax.py to softmax.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/misc.py to misc.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/embedding.py to embedding.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/base.py to base.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/utility.py to utility.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/__main__.py to __main__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/convert.py to convert.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/loss.py to loss.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/pooling.py to pooling.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/prof/reduction.py to reduction.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse/db.py to db.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse/kernel.py to kernel.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse/__main__.py to __main__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/parse/parse.py to parse.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/_autocast_utils.py to _autocast_utils.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/bottleneck/bottleneck_module_test.py to bottleneck_module_test.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/bottleneck/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/bottleneck/test.py to test.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/layer_norm/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/sparsity/asp.py to asp.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/transducer/transducer.py to transducer.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/transducer/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/fmha/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/fmha/fmha.py to fmha.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-38.pyc\n",
      "    byte-compiling /opt/conda/lib/python3.8/site-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-38.pyc\n",
      "    running install_egg_info\n",
      "    running egg_info\n",
      "    creating apex.egg-info\n",
      "    writing apex.egg-info/PKG-INFO\n",
      "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
      "    writing top-level names to apex.egg-info/top_level.txt\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "    reading manifest file 'apex.egg-info/SOURCES.txt'\n",
      "    adding license file 'LICENSE'\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "    Copying apex.egg-info to /opt/conda/lib/python3.8/site-packages/apex-0.1-py3.8.egg-info\n",
      "    running install_scripts\n",
      "    writing list of installed files to '/tmp/pip-record-7ong8nue/install-record.txt'\n",
      "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
      "Successfully installed apex-0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /notebooks/\n",
    "#!git clone https://github.com/NVIDIA/apex\n",
    "%cd apex\n",
    "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc-EidleBfeB"
   },
   "source": [
    "## Install PyCocoTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "JwuWoBOxFV6v",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e547af2c-4dad-48cf-94e8-8ac9eaf15393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.24)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
      "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-ztnex1fy\n",
      "  Running command git clone --filter=blob:none -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-ztnex1fy\n",
      "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (57.4.0)\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (0.29.24)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (3.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0-cp37-cp37m-linux_x86_64.whl size=263926 sha256=90df649e9e71af0f554067828bc40a03d17cccabd1f953ac9f5de15cdd99d49c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4f5u9g5s/wheels/e2/6b/1d/344ac773c7495ea0b85eb228bc66daec7400a143a92d36b7b1\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: pycocotools\n",
      "  Attempting uninstall: pycocotools\n",
      "    Found existing installation: pycocotools 2.0.2\n",
      "    Uninstalling pycocotools-2.0.2:\n",
      "      Successfully uninstalled pycocotools-2.0.2\n",
      "Successfully installed pycocotools-2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install cython; pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEdiT0rJBmRA"
   },
   "source": [
    "# Download your Data\n",
    "\n",
    "We'll download our dataset from Roboflow. Use the \"**Pascal VOC**\" export format.\n",
    "\n",
    "To get your data into Roboflow, follow the [Getting Started Guide](https://blog.roboflow.ai/getting-started-with-roboflow/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gp1L8zdwGo_j",
    "outputId": "41d52610-0604-497f-ad64-71616b6aa856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/YOLOX\n"
     ]
    }
   ],
   "source": [
    "#%cd /notebooks/\n",
    "#!curl -L \"https://app.roboflow.com/ds/EBPiJKRSg3?key=E1k7623vY4\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
    "\n",
    "%cd /notebooks/YOLOX/\n",
    "#!ln -s /notebooks/train/ ./datasets/VOCdevkit\n",
    "!rm -rf datasets/VOCdevkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gp1L8zdwGo_j",
    "outputId": "41d52610-0604-497f-ad64-71616b6aa856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/YOLOX\n"
     ]
    }
   ],
   "source": [
    "#%cd /notebooks/\n",
    "#!curl -L \"https://app.roboflow.com/ds/EBPiJKRSg3?key=E1k7623vY4\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
    "\n",
    "%cd YOLOX/\n",
    "!ln -s /notebooks/train/ ./datasets/VOCdevkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agZSFjXLByrv"
   },
   "source": [
    "## Format Your Data Appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xTRtDWrIw_D",
    "outputId": "53dd4443-80fc-4cfd-bc5d-a2bf14bbe55b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and val size: 1779\n",
      "train size: 1423\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p \"/notebooks/YOLOX/datasets/VOCdevkit/VOC2007\"\n",
    "!python3 voc_txt.py \"/notebooks/YOLOX/datasets/VOCdevkit/\"\n",
    "%mkdir -p \"/notebooks/YOLOX/datasets/VOCdevkit/VOC2012\"\n",
    "!cp -r \"/notebooks/YOLOX/datasets/VOCdevkit/VOC2007/.\" \"/notebooks/YOLOX/datasets/VOCdevkit/VOC2012\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW8iyuMyB3bc"
   },
   "source": [
    "## Change the Classes\n",
    "Make sure you change the classes based on what your dataset. To ensure that the training process will function as intended, write the classes in lowercase with no whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rohuAE541Nug"
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9h5PM8Ft1OjG",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##REPLACE this cell with your classnames stripped of whitespace and lowercase\n",
    "%%writetemplate /content/YOLOX/yolox/data/datasets/voc_classes.py\n",
    "\n",
    "VOC_CLASSES = (\n",
    "  \"bed\",\n",
    "  \"pillow\",\n",
    "  \"swimming-pool\",\n",
    "  \"television\",\n",
    "  \"sofa-bed\",\n",
    "  \"sink\",\n",
    "  \"porch\",\n",
    "  \"stairs\",\n",
    "  \"kitchen-&-dining-room-table\",\n",
    "  \"billiard-table\",\n",
    "  \"couch\",\n",
    "  \"toilet\",\n",
    "  \"fountain\",\n",
    "  \"washing-machine\",\n",
    "  \"mirror\",\n",
    "  \"oven\",\n",
    "  \"countertop\",\n",
    "  \"fireplace\",\n",
    "  \"refrigerator\",\n",
    "  \"microwave-oven\",\n",
    "  \"ceiling-fan\",\n",
    "  \"bathtub\",\n",
    "  \"coffeemaker\",\n",
    "  \"gas-stove\",\n",
    "  \"towel\",\n",
    "  \"shower\",\n",
    "  \"wine-rack\",\n",
    "  \"jacuzzi\",\n",
    "  \"tree-house\",\n",
    "  \"dishwasher\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Lu6_LzErQRSU",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##REPLACE this cell with your classnames stripped of whitespace and lowercase\n",
    "%%writetemplate /content/YOLOX/yolox/data/datasets/coco_classes.py\n",
    "\n",
    "COCO_CLASSES = (\n",
    "  \"bed\",\n",
    "  \"pillow\",\n",
    "  \"swimming-pool\",\n",
    "  \"television\",\n",
    "  \"sofa-bed\",\n",
    "  \"sink\",\n",
    "  \"porch\",\n",
    "  \"stairs\",\n",
    "  \"kitchen-&-dining-room-table\",\n",
    "  \"billiard-table\",\n",
    "  \"couch\",\n",
    "  \"toilet\",\n",
    "  \"fountain\",\n",
    "  \"washing-machine\",\n",
    "  \"mirror\",\n",
    "  \"oven\",\n",
    "  \"countertop\",\n",
    "  \"fireplace\",\n",
    "  \"refrigerator\",\n",
    "  \"microwave-oven\",\n",
    "  \"ceiling-fan\",\n",
    "  \"bathtub\",\n",
    "  \"coffeemaker\",\n",
    "  \"gas-stove\",\n",
    "  \"towel\",\n",
    "  \"shower\",\n",
    "  \"wine-rack\",\n",
    "  \"jacuzzi\",\n",
    "  \"tree-house\",\n",
    "  \"dishwasher\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uAaf5AKSE_E"
   },
   "source": [
    "Set the number of classes you have in your dataset in te `NUM_CLASSES` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "hxA0JmWqwU_M",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 30\n",
    "!sed -i -e 's/self.num_classes = 20/self.num_classes = {NUM_CLASSES}/g' \"/notebooks/YOLOX/exps/example/yolox_voc/yolox_voc_s.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiYvw_GGKaro"
   },
   "source": [
    "# Download Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "UsOCh9hRKbIw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "97ee0c85-26af-4a68-bdca-87a7c61caad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "--2021-11-30 06:08:51--  https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-releases.githubusercontent.com/388351473/0b307dd4-bddb-4cfe-a863-1d19afb5598a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211130T060851Z&X-Amz-Expires=300&X-Amz-Signature=e9131ea2b239579c42bf4b312815715bd8652b83d24ffbceeab536c574879dfd&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=388351473&response-content-disposition=attachment%3B%20filename%3Dyolox_s.pth&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-11-30 06:08:51--  https://github-releases.githubusercontent.com/388351473/0b307dd4-bddb-4cfe-a863-1d19afb5598a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211130T060851Z&X-Amz-Expires=300&X-Amz-Signature=e9131ea2b239579c42bf4b312815715bd8652b83d24ffbceeab536c574879dfd&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=388351473&response-content-disposition=attachment%3B%20filename%3Dyolox_s.pth&response-content-type=application%2Foctet-stream\n",
      "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 72050245 (69M) [application/octet-stream]\n",
      "Saving to: ‘yolox_s.pth’\n",
      "\n",
      "yolox_s.pth         100%[===================>]  68.71M  76.5MB/s    in 0.9s    \n",
      "\n",
      "2021-11-30 06:08:52 (76.5 MB/s) - ‘yolox_s.pth’ saved [72050245/72050245]\n",
      "\n",
      "--2021-11-30 06:08:52--  https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_m.pth\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/388351473/a6db9498-8e22-4ea7-8bc5-2b3125c46245?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211130T060852Z&X-Amz-Expires=300&X-Amz-Signature=b90056f6d455d04e21426d6ca7e192af80d1999de0f7f57495eb35b4142ca5da&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=388351473&response-content-disposition=attachment%3B%20filename%3Dyolox_m.pth&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-11-30 06:08:52--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/388351473/a6db9498-8e22-4ea7-8bc5-2b3125c46245?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211130T060852Z&X-Amz-Expires=300&X-Amz-Signature=b90056f6d455d04e21426d6ca7e192af80d1999de0f7f57495eb35b4142ca5da&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=388351473&response-content-disposition=attachment%3B%20filename%3Dyolox_m.pth&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203063317 (194M) [application/octet-stream]\n",
      "Saving to: ‘yolox_m.pth’\n",
      "\n",
      "yolox_m.pth         100%[===================>] 193.66M  77.4MB/s    in 2.5s    \n",
      "\n",
      "2021-11-30 06:08:55 (77.4 MB/s) - ‘yolox_m.pth’ saved [203063317/203063317]\n",
      "\n",
      "--2021-11-30 06:08:55--  https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_x.pth\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-releases.githubusercontent.com/388351473/77a2128d-8fad-4181-a754-0daf70511100?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211130T060856Z&X-Amz-Expires=300&X-Amz-Signature=41e7c7e735858bab78140ff1b703d4655f2a43dee3f203dcbe850a7cbaf54352&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=388351473&response-content-disposition=attachment%3B%20filename%3Dyolox_x.pth&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-11-30 06:08:56--  https://github-releases.githubusercontent.com/388351473/77a2128d-8fad-4181-a754-0daf70511100?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211130T060856Z&X-Amz-Expires=300&X-Amz-Signature=41e7c7e735858bab78140ff1b703d4655f2a43dee3f203dcbe850a7cbaf54352&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=388351473&response-content-disposition=attachment%3B%20filename%3Dyolox_x.pth&response-content-type=application%2Foctet-stream\n",
      "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 793388371 (757M) [application/octet-stream]\n",
      "Saving to: ‘yolox_x.pth’\n",
      "\n",
      "yolox_x.pth         100%[===================>] 756.63M  54.5MB/s    in 13s     \n",
      "\n",
      "2021-11-30 06:09:09 (57.5 MB/s) - ‘yolox_x.pth’ saved [793388371/793388371]\n",
      "\n",
      "/content/YOLOX\n"
     ]
    }
   ],
   "source": [
    "%cd /content/\n",
    "!wget https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth\n",
    "!wget https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_m.pth\n",
    "!wget https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_x.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TabCpJOCRti"
   },
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1J2AWY-4lj2"
   },
   "source": [
    "Copy yolox_m and yolox_x into the yolox_voc/ directory. You have to manually copy the dataloader related code from yolox_voc_s.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "uDUuOaP3u5AE",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/YOLOX/'\n",
      "/notebooks/YOLOX\n"
     ]
    }
   ],
   "source": [
    "%cd /notebooks/YOLOX/\n",
    "!cp exps/default/yolox_m.py exps/example/yolox_voc/yolox_voc_m.py\n",
    "!cp exps/default/yolox_x.py exps/example/yolox_voc/yolox_voc_x.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "s5h536amH32Z",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b99cb8b1-97a1-492b-adab-2c7c764d719b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/YOLOX\n",
      "\u001b[32m2021-12-01 22:10:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.core.trainer\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1margs: Namespace(batch_size=32, ckpt='/notebooks/yolox_m.pth', devices=1, dist_backend='nccl', dist_url=None, exp_file='exps/example/yolox_voc/yolox_voc_m.py', experiment_name='variant-exp-yolox-m', fp16=True, local_rank=0, machine_rank=0, name='yolox-m', num_machines=1, occupy=True, opts=['max_epoch', '30'], resume=False, start_epoch=None)\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.core.trainer\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mexp value:\n",
      "╒══════════════════╤════════════════════════════╕\n",
      "│ keys             │ values                     │\n",
      "╞══════════════════╪════════════════════════════╡\n",
      "│ seed             │ None                       │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ output_dir       │ './YOLOX_outputs'          │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ print_interval   │ 10                         │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ eval_interval    │ 10                         │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ num_classes      │ 30                         │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ depth            │ 0.67                       │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ width            │ 0.75                       │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ data_num_workers │ 4                          │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ input_size       │ (640, 640)                 │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ random_size      │ (14, 26)                   │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ train_ann        │ 'instances_train2017.json' │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ val_ann          │ 'instances_val2017.json'   │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ degrees          │ 10.0                       │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ translate        │ 0.1                        │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ scale            │ (0.1, 2)                   │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ mscale           │ (0.8, 1.6)                 │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ shear            │ 2.0                        │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ perspective      │ 0.0                        │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ enable_mixup     │ True                       │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ warmup_epochs    │ 5                          │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ max_epoch        │ 30                         │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ warmup_lr        │ 0                          │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ basic_lr_per_img │ 0.00015625                 │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ scheduler        │ 'yoloxwarmcos'             │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ no_aug_epochs    │ 15                         │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ min_lr_ratio     │ 0.05                       │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ ema              │ True                       │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ weight_decay     │ 0.0005                     │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ momentum         │ 0.9                        │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ exp_name         │ 'yolox_voc_m'              │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ test_size        │ (640, 640)                 │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ test_conf        │ 0.01                       │\n",
      "├──────────────────┼────────────────────────────┤\n",
      "│ nmsthre          │ 0.65                       │\n",
      "╘══════════════════╧════════════════════════════╛\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.core.trainer\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mModel Summary: Params: 25.30M, Gflops: 73.59\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mSelected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m329\u001b[0m - \u001b[1mDefaults for this optimization level are:\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1menabled                : True\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1mopt_level              : O1\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1mcast_model_type        : None\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1mpatch_torch_functions  : True\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1mkeep_batchnorm_fp32    : None\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1mmaster_weights         : None\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m331\u001b[0m - \u001b[1mloss_scale             : dynamic\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mProcessing user overrides (additional kwargs that are not None)...\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m354\u001b[0m - \u001b[1mAfter processing overrides, optimization options are:\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1menabled                : True\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mopt_level              : O1\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mcast_model_type        : None\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mpatch_torch_functions  : True\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mkeep_batchnorm_fp32    : None\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mmaster_weights         : None\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.frontend\u001b[0m:\u001b[36m356\u001b[0m - \u001b[1mloss_scale             : dynamic\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.core.trainer\u001b[0m:\u001b[36m306\u001b[0m - \u001b[1mloading checkpoint for fine tuning\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36myolox.utils.checkpoint\u001b[0m:\u001b[36m25\u001b[0m - \u001b[33m\u001b[1mShape of head.cls_preds.0.weight in checkpoint is torch.Size([80, 192, 1, 1]), while shape of head.cls_preds.0.weight in model is torch.Size([30, 192, 1, 1]).\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36myolox.utils.checkpoint\u001b[0m:\u001b[36m25\u001b[0m - \u001b[33m\u001b[1mShape of head.cls_preds.0.bias in checkpoint is torch.Size([80]), while shape of head.cls_preds.0.bias in model is torch.Size([30]).\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36myolox.utils.checkpoint\u001b[0m:\u001b[36m25\u001b[0m - \u001b[33m\u001b[1mShape of head.cls_preds.1.weight in checkpoint is torch.Size([80, 192, 1, 1]), while shape of head.cls_preds.1.weight in model is torch.Size([30, 192, 1, 1]).\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36myolox.utils.checkpoint\u001b[0m:\u001b[36m25\u001b[0m - \u001b[33m\u001b[1mShape of head.cls_preds.1.bias in checkpoint is torch.Size([80]), while shape of head.cls_preds.1.bias in model is torch.Size([30]).\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36myolox.utils.checkpoint\u001b[0m:\u001b[36m25\u001b[0m - \u001b[33m\u001b[1mShape of head.cls_preds.2.weight in checkpoint is torch.Size([80, 192, 1, 1]), while shape of head.cls_preds.2.weight in model is torch.Size([30, 192, 1, 1]).\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36myolox.utils.checkpoint\u001b[0m:\u001b[36m25\u001b[0m - \u001b[33m\u001b[1mShape of head.cls_preds.2.bias in checkpoint is torch.Size([80]), while shape of head.cls_preds.2.bias in model is torch.Size([30]).\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.core.trainer\u001b[0m:\u001b[36m152\u001b[0m - \u001b[1minit prefetcher, this might take one minute or less...\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.core.trainer\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mTraining start...\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.core.trainer\u001b[0m:\u001b[36m183\u001b[0m - \u001b[1m\n",
      "YOLOX(\n",
      "  (backbone): YOLOPAFPN(\n",
      "    (backbone): CSPDarknet(\n",
      "      (stem): Focus(\n",
      "        (conv): BaseConv(\n",
      "          (conv): Conv2d(12, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (dark2): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dark3): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (4): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (5): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dark4): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (4): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (5): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dark5): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): SPPBottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): ModuleList(\n",
      "            (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "            (1): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
      "            (2): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (lateral_conv0): BaseConv(\n",
      "      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_p4): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reduce_conv1): BaseConv(\n",
      "      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_p3): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bu_conv2): BaseConv(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_n3): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bu_conv1): BaseConv(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_n4): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): YOLOXHead(\n",
      "    (cls_convs): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reg_convs): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls_preds): ModuleList(\n",
      "      (0): Conv2d(192, 30, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(192, 30, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(192, 30, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (reg_preds): ModuleList(\n",
      "      (0): Conv2d(192, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(192, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(192, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (obj_preds): ModuleList(\n",
      "      (0): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (stems): ModuleList(\n",
      "      (0): BaseConv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv(\n",
      "        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): BaseConv(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (l1_loss): L1Loss()\n",
      "    (bcewithlog_loss): BCEWithLogitsLoss()\n",
      "    (iou_loss): IOUloss()\n",
      "  )\n",
      ")\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.core.trainer\u001b[0m:\u001b[36m193\u001b[0m - \u001b[1m---> start train epoch1\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.handle\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1mGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapex.amp.handle\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1mGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.core.trainer\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1mepoch: 1/30, iter: 10/112, mem: 13229Mb, iter_time: 1.304s, data_time: 0.004s, total_loss: 14.4, iou_loss: 2.1, l1_loss: 0.0, conf_loss: 8.2, cls_loss: 4.1, lr: 1.594e-06, size: 640, ETA: 1:12:49\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.core.trainer\u001b[0m:\u001b[36m186\u001b[0m - \u001b[1mTraining of experiment is done and the best AP is 0.00\u001b[0m\n",
      "\u001b[32m2021-12-01 22:10:37\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36myolox.core.launch\u001b[0m:\u001b[36m90\u001b[0m - \u001b[31m\u001b[1mAn error has been caught in function 'launch', process 'MainProcess' (5842), thread 'MainThread' (139923907360576):\u001b[0m\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"\u001b[32mtools/\u001b[0m\u001b[32m\u001b[1mtrain.py\u001b[0m\", line \u001b[33m118\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[1mlaunch\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<function launch at 0x7f41f3242ca0>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/notebooks/YOLOX/yolox/core/\u001b[0m\u001b[32m\u001b[1mlaunch.py\u001b[0m\", line \u001b[33m90\u001b[0m, in \u001b[35mlaunch\u001b[0m\n",
      "    \u001b[1mmain_func\u001b[0m\u001b[1m(\u001b[0m\u001b[35m\u001b[1m*\u001b[0m\u001b[1margs\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│          └ \u001b[0m\u001b[36m\u001b[1m(╒══════════════════╤════════════════════════════════════════════════════════════════════════════════════════════════════════...\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<function main at 0x7f41e3671f70>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32mtools/\u001b[0m\u001b[32m\u001b[1mtrain.py\u001b[0m\", line \u001b[33m104\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[1mtrainer\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mtrain\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│       └ \u001b[0m\u001b[36m\u001b[1m<function Trainer.train at 0x7f41e5bd5e50>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<yolox.core.trainer.Trainer object at 0x7f41e367b4f0>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/notebooks/YOLOX/yolox/core/\u001b[0m\u001b[32m\u001b[1mtrainer.py\u001b[0m\", line \u001b[33m71\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "    \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mtrain_in_epoch\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│    └ \u001b[0m\u001b[36m\u001b[1m<function Trainer.train_in_epoch at 0x7f41e36fa5e0>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<yolox.core.trainer.Trainer object at 0x7f41e367b4f0>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/notebooks/YOLOX/yolox/core/\u001b[0m\u001b[32m\u001b[1mtrainer.py\u001b[0m\", line \u001b[33m80\u001b[0m, in \u001b[35mtrain_in_epoch\u001b[0m\n",
      "    \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mtrain_in_iter\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│    └ \u001b[0m\u001b[36m\u001b[1m<function Trainer.train_in_iter at 0x7f41e3703700>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<yolox.core.trainer.Trainer object at 0x7f41e367b4f0>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/notebooks/YOLOX/yolox/core/\u001b[0m\u001b[32m\u001b[1mtrainer.py\u001b[0m\", line \u001b[33m86\u001b[0m, in \u001b[35mtrain_in_iter\u001b[0m\n",
      "    \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mtrain_one_iter\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│    └ \u001b[0m\u001b[36m\u001b[1m<function Trainer.train_one_iter at 0x7f41e3703790>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<yolox.core.trainer.Trainer object at 0x7f41e367b4f0>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/notebooks/YOLOX/yolox/core/\u001b[0m\u001b[32m\u001b[1mtrainer.py\u001b[0m\", line \u001b[33m92\u001b[0m, in \u001b[35mtrain_one_iter\u001b[0m\n",
      "    \u001b[1minps\u001b[0m\u001b[1m,\u001b[0m \u001b[1mtargets\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mprefetcher\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mnext\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                │    │          └ \u001b[0m\u001b[36m\u001b[1m<function DataPrefetcher.next at 0x7f41e5bd5ca0>\u001b[0m\n",
      "    \u001b[36m                │    └ \u001b[0m\u001b[36m\u001b[1m<yolox.data.data_prefetcher.DataPrefetcher object at 0x7f41dadfa040>\u001b[0m\n",
      "    \u001b[36m                └ \u001b[0m\u001b[36m\u001b[1m<yolox.core.trainer.Trainer object at 0x7f41e367b4f0>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/notebooks/YOLOX/yolox/data/\u001b[0m\u001b[32m\u001b[1mdata_prefetcher.py\u001b[0m\", line \u001b[33m48\u001b[0m, in \u001b[35mnext\u001b[0m\n",
      "    \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mpreload\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│    └ \u001b[0m\u001b[36m\u001b[1m<function DataPrefetcher.preload at 0x7f41e5bd5c10>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<yolox.data.data_prefetcher.DataPrefetcher object at 0x7f41dadfa040>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/notebooks/YOLOX/yolox/data/\u001b[0m\u001b[32m\u001b[1mdata_prefetcher.py\u001b[0m\", line \u001b[33m30\u001b[0m, in \u001b[35mpreload\u001b[0m\n",
      "    \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mnext_input\u001b[0m\u001b[1m,\u001b[0m \u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mnext_target\u001b[0m\u001b[1m,\u001b[0m \u001b[1m_\u001b[0m\u001b[1m,\u001b[0m \u001b[1m_\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mnext\u001b[0m\u001b[1m(\u001b[0m\u001b[1mself\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mloader\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│    │           │    │                        │    └ \u001b[0m\u001b[36m\u001b[1m<torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f41dade7250>\u001b[0m\n",
      "    \u001b[36m│    │           │    │                        └ \u001b[0m\u001b[36m\u001b[1m<yolox.data.data_prefetcher.DataPrefetcher object at 0x7f41dadfa040>\u001b[0m\n",
      "    \u001b[36m│    │           │    └ \u001b[0m\u001b[36m\u001b[1mtensor([[[  5.0000, 312.1171, 259.6220, 287.0510, 148.0453],\u001b[0m\n",
      "    \u001b[36m│    │           │      \u001b[0m\u001b[36m\u001b[1m         [  3.0000, 256.7303, 215.7497, 360.7123, 431.4995],\u001b[0m\n",
      "    \u001b[36m│    │           │      \u001b[0m\u001b[36m\u001b[1m   ...\u001b[0m\n",
      "    \u001b[36m│    │           └ \u001b[0m\u001b[36m\u001b[1m<yolox.data.data_prefetcher.DataPrefetcher object at 0x7f41dadfa040>\u001b[0m\n",
      "    \u001b[36m│    └ \u001b[0m\u001b[36m\u001b[1mtensor([[[[-1.4658, -1.4740, -1.5649,  ..., -1.0528, -1.0775, -1.0858],\u001b[0m\n",
      "    \u001b[36m│      \u001b[0m\u001b[36m\u001b[1m          [-1.4658, -1.4823, -1.5731,  ..., -1.0362, ...\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<yolox.data.data_prefetcher.DataPrefetcher object at 0x7f41dadfa040>\u001b[0m\n",
      "\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
      "    data = self._next_data()\n",
      "           │    └ <function _MultiProcessingDataLoaderIter._next_data at 0x7f41f322e040>\n",
      "           └ <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f41dade7250>\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1065, in _next_data\n",
      "    return self._process_data(data)\n",
      "           │    │             └ <torch._utils.ExceptionWrapper object at 0x7f41d8464fa0>\n",
      "           │    └ <function _MultiProcessingDataLoaderIter._process_data at 0x7f41f322e160>\n",
      "           └ <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f41dade7250>\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1111, in _process_data\n",
      "    data.reraise()\n",
      "    │    └ <function ExceptionWrapper.reraise at 0x7f4292093b80>\n",
      "    └ <torch._utils.ExceptionWrapper object at 0x7f41d8464fa0>\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/_utils.py\", line 428, in reraise\n",
      "    raise self.exc_type(msg)\n",
      "          │    │        └ 'Caught ParseError in DataLoader worker process 3.\\nOriginal Traceback (most recent call last):\\n  File \"/opt/conda/lib/pytho...\n",
      "          │    └ <class 'xml.etree.ElementTree.ParseError'>\n",
      "          └ <torch._utils.ExceptionWrapper object at 0x7f41d8464fa0>\n",
      "\n",
      "\u001b[31m\u001b[1mxml.etree.ElementTree.ParseError\u001b[0m:\u001b[1m Caught ParseError in DataLoader worker process 3.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/notebooks/YOLOX/yolox/data/datasets/datasets_wrapper.py\", line 121, in wrapper\n",
      "    ret_val = getitem_fn(self, index)\n",
      "  File \"/notebooks/YOLOX/yolox/data/datasets/mosaicdetection.py\", line 91, in __getitem__\n",
      "    img, _labels, _, _ = self._dataset.pull_item(index)\n",
      "  File \"/notebooks/YOLOX/yolox/data/datasets/voc.py\", line 145, in pull_item\n",
      "    target = self.load_anno(index)\n",
      "  File \"/notebooks/YOLOX/yolox/data/datasets/voc.py\", line 124, in load_anno\n",
      "    target = ET.parse(self._annopath % img_id).getroot()\n",
      "  File \"/opt/conda/lib/python3.8/xml/etree/ElementTree.py\", line 1202, in parse\n",
      "    tree.parse(source, parser)\n",
      "  File \"/opt/conda/lib/python3.8/xml/etree/ElementTree.py\", line 595, in parse\n",
      "    self._root = parser._parse_whole(source)\n",
      "xml.etree.ElementTree.ParseError: no element found: line 1, column 0\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd /notebooks/YOLOX\n",
    "EXP_NAME=\"variant-exp-yolox-m\"\n",
    "EXP_DESC_FILE=\"exps/example/yolox_voc/yolox_voc_m.py\"\n",
    "MODEL_NAME=\"yolox-m\"\n",
    "YOLOX_PATH=\"/notebooks/yolox_m.pth\"\n",
    "MODEL_PATH = \"/notebooks/YOLOX/YOLOX_outputs/variant-exp-yolox-m/latest_ckpt.pth.tar\"\n",
    "\n",
    "!python tools/train.py -expn {EXP_NAME} -n {MODEL_NAME} -b 32 -d 1 -f {EXP_DESC_FILE} -c {YOLOX_PATH} --fp16 -o max_epoch 30\n",
    "#!python tools/train.py -expn {EXP_NAME} -n {MODEL_NAME} -b 16 -d 1 -f {EXP_DESC_FILE} -c {MODEL_PATH} --resume --fp16 -o max_epoch 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UjsuFDICVov"
   },
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sE5oWuEOICAF",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "08c34eed-87eb-49d9-e2a9-56c890832b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2021-11-30 11:57:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m129\u001b[0m - \u001b[1mArgs: Namespace(batch_size=64, ckpt='/content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/best_ckpt.pth.tar', conf=0.001, devices=1, dist_backend='nccl', dist_url=None, exp_file='exps/example/yolox_voc/yolox_voc_m.py', experiment_name='yolox_voc_m', fp16=False, fuse=False, local_rank=0, machine_rank=0, name='yolox-s', nms=None, num_machines=1, opts=[], seed=None, speed=False, test=False, trt=False, tsize=None)\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001b[32m2021-11-30 11:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m139\u001b[0m - \u001b[1mModel Summary: Params: 25.33M, Gflops: 73.76\u001b[0m\n",
      "\u001b[32m2021-11-30 11:57:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mModel Structure:\n",
      "YOLOX(\n",
      "  (backbone): YOLOPAFPN(\n",
      "    (backbone): CSPDarknet(\n",
      "      (stem): Focus(\n",
      "        (conv): BaseConv(\n",
      "          (conv): Conv2d(12, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (dark2): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dark3): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (4): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (5): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dark4): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (4): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (5): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dark5): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): SPPBottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): ModuleList(\n",
      "            (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "            (1): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
      "            (2): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (2): CSPLayer(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv3): BaseConv(\n",
      "            (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): BaseConv(\n",
      "                (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (conv2): BaseConv(\n",
      "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (lateral_conv0): BaseConv(\n",
      "      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_p4): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reduce_conv1): BaseConv(\n",
      "      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_p3): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bu_conv2): BaseConv(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_n3): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bu_conv1): BaseConv(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (C3_n4): CSPLayer(\n",
      "      (conv1): BaseConv(\n",
      "        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv2): BaseConv(\n",
      "        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (conv3): BaseConv(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): BaseConv(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv2): BaseConv(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): YOLOXHead(\n",
      "    (cls_convs): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (reg_convs): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): BaseConv(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls_preds): ModuleList(\n",
      "      (0): Conv2d(192, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(192, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(192, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (reg_preds): ModuleList(\n",
      "      (0): Conv2d(192, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(192, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(192, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (obj_preds): ModuleList(\n",
      "      (0): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(192, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (stems): ModuleList(\n",
      "      (0): BaseConv(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): BaseConv(\n",
      "        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): BaseConv(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (l1_loss): L1Loss()\n",
      "    (bcewithlog_loss): BCEWithLogitsLoss()\n",
      "    (iou_loss): IOUloss()\n",
      "  )\n",
      ")\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\u001b[32m2021-11-30 11:57:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mloading checkpoint\u001b[0m\n",
      "\u001b[32m2021-11-30 11:57:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mloaded checkpoint done.\u001b[0m\n",
      "100%|##########| 4/4 [00:33<00:00,  8.49s/it]\n",
      "\u001b[32m2021-11-30 11:58:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36myolox.evaluators.voc_evaluator\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mEvaluate in main process...\u001b[0m\n",
      "Writing bed VOC results file\n",
      "Writing pillow VOC results file\n",
      "Writing swimming-pool VOC results file\n",
      "Writing television VOC results file\n",
      "Writing sofa-bed VOC results file\n",
      "Writing sink VOC results file\n",
      "Writing porch VOC results file\n",
      "Writing stairs VOC results file\n",
      "Writing kitchen-&-dining-room-table VOC results file\n",
      "Writing billiard-table VOC results file\n",
      "Writing couch VOC results file\n",
      "Writing toilet VOC results file\n",
      "Writing fountain VOC results file\n",
      "Writing washing-machine VOC results file\n",
      "Writing mirror VOC results file\n",
      "Writing oven VOC results file\n",
      "Writing countertop VOC results file\n",
      "Writing fireplace VOC results file\n",
      "Writing refrigerator VOC results file\n",
      "Writing microwave-oven VOC results file\n",
      "Writing ceiling-fan VOC results file\n",
      "Writing bathtub VOC results file\n",
      "Writing coffeemaker VOC results file\n",
      "Writing gas-stove VOC results file\n",
      "Writing towel VOC results file\n",
      "Writing shower VOC results file\n",
      "Writing wine-rack VOC results file\n",
      "Writing jacuzzi VOC results file\n",
      "Writing tree-house VOC results file\n",
      "Writing dishwasher VOC results file\n",
      "Eval IoU : 0.50\n",
      "AP for bed = 0.6733\n",
      "AP for pillow = 0.0000\n",
      "AP for swimming-pool = 0.8345\n",
      "AP for television = 0.7027\n",
      "AP for sofa-bed = 0.0000\n",
      "AP for sink = 0.4908\n",
      "AP for porch = 0.0000\n",
      "AP for stairs = 0.0000\n",
      "AP for kitchen-&-dining-room-table = 0.0000\n",
      "AP for billiard-table = 0.6620\n",
      "AP for couch = 0.0000\n",
      "AP for toilet = 0.7437\n",
      "AP for fountain = 0.5071\n",
      "AP for washing-machine = 0.0000\n",
      "AP for mirror = 0.0000\n",
      "AP for oven = 0.8042\n",
      "AP for countertop = 0.0000\n",
      "AP for fireplace = 0.0000\n",
      "AP for refrigerator = 0.0000\n",
      "AP for microwave-oven = 0.5920\n",
      "AP for ceiling-fan = 0.7013\n",
      "AP for bathtub = 0.0000\n",
      "AP for coffeemaker = 0.0000\n",
      "AP for gas-stove = 0.0000\n",
      "AP for towel = 0.0000\n",
      "AP for shower = 0.0000\n",
      "AP for wine-rack = 0.0000\n",
      "AP for jacuzzi = 0.0000\n",
      "AP for tree-house = 0.0000\n",
      "AP for dishwasher = 0.0000\n",
      "Mean AP = 0.2237\n",
      "~~~~~~~~\n",
      "Results:\n",
      "0.673\n",
      "0.000\n",
      "0.834\n",
      "0.703\n",
      "0.000\n",
      "0.491\n",
      "0.000\n",
      "0.000\n",
      "0.000\n",
      "0.662\n",
      "0.000\n",
      "0.744\n",
      "0.507\n",
      "0.000\n",
      "0.000\n",
      "0.804\n",
      "0.000\n",
      "0.000\n",
      "0.000\n",
      "0.592\n",
      "0.701\n",
      "0.000\n",
      "0.000\n",
      "0.000\n",
      "0.000\n",
      "0.000\n",
      "0.000\n",
      "0.000\n",
      "0.000\n",
      "0.000\n",
      "0.224\n",
      "~~~~~~~~\n",
      "\n",
      "--------------------------------------------------------------\n",
      "Results computed with the **unofficial** Python eval code.\n",
      "Results should be very close to the official MATLAB eval code.\n",
      "Recompute with `./tools/reval.py --matlab ...` for your paper.\n",
      "-- Thanks, The Management\n",
      "--------------------------------------------------------------\n",
      "Eval IoU : 0.55\n",
      "Eval IoU : 0.60\n",
      "Eval IoU : 0.65\n",
      "Eval IoU : 0.70\n",
      "Eval IoU : 0.75\n",
      "Eval IoU : 0.80\n",
      "Eval IoU : 0.85\n",
      "Eval IoU : 0.90\n",
      "Eval IoU : 0.95\n",
      "--------------------------------------------------------------\n",
      "map_5095: 0.15277035744215994\n",
      "map_50: 0.22372174458576596\n",
      "--------------------------------------------------------------\n",
      "\u001b[32m2021-11-30 11:58:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m185\u001b[0m - \u001b[1m\n",
      "Average forward time: 101.09 ms, Average NMS time: 0.98 ms, Average inference time: 102.08 ms\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"/content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/best_ckpt.pth.tar\"\n",
    "!python3 tools/eval.py -n  yolox-s -c {MODEL_PATH} -b 64 -d 1 --conf 0.001 -f exps/example/yolox_voc/yolox_voc_m.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnFRJEa7CaPe"
   },
   "source": [
    "# Test the Model\n",
    "Make sure you replace the `TEST_IMAGE_PATH` variable with a test image from your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Smh6Yh4vIYor",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1 validation image from each class\n",
    "TEST_IMAGE_PATHS = [\n",
    "                    \"/content/valid/ead5ec887d5c02a7_jpg.rf.b8c9fb402791dc4cf384476828bfd2d4.jpg\",\n",
    "                    \"/content/valid/fb81bce006d17076_jpg.rf.48415aa14ccfdd3497eb20783b5644d6.jpg\",\n",
    "                    \"/content/valid/94778d10168b17b0_jpg.rf.eaa84eb925565613161688f6487fc33c.jpg\",\n",
    "                    \"/content/valid/08e5905e139f6425_jpg.rf.d58a09d87843c60e9f1897a94fdc44a1.jpg\",\n",
    "                    \"/content/valid/118cfaa4ead8036c_jpg.rf.ccf53544a798623be898ede526c24930.jpg\",\n",
    "                    \"/content/valid/bb6502ebcd533137_jpg.rf.88dc373d5a9741a7a5f118581f0137aa.jpg\",\n",
    "                    \"/content/valid/82af8ecf17d42a78_jpg.rf.778833c8052abbe3ee3f4e9e58870d63.jpg\",\n",
    "                    \"/content/valid/040659777cc851a1_jpg.rf.861a6d222877d7dede55a802885e7492.jpg\",\n",
    "                    \"/content/valid/05c680512a087536_jpg.rf.4da802147647e2b70427a54c417be2e0.jpg\",\n",
    "                    \"/content/valid/ca7a544fb7edb497_jpg.rf.87aebb71865dec3a490f92e02e2440e9.jpg\",\n",
    "                    \"/content/valid/1c11f389a2f2673a_jpg.rf.231637a7a032bd38345e6804d9ed312f.jpg\",\n",
    "                    \"/content/valid/ca7a544fb7edb497_jpg.rf.87aebb71865dec3a490f92e02e2440e9.jpg\",\n",
    "                    \"/content/valid/016fbd083b98786f_jpg.rf.f9a56b569101cb481053060c3ba5e296.jpg\",\n",
    "                    \"/content/valid/60628c08dffbb0ed_jpg.rf.32760210054dc74c2fd97967bc192a38.jpg\",\n",
    "                    \"/content/valid/49f2147514665f24_jpg.rf.ce41e445f8c6276ae2bd0167c066cf5a.jpg\",\n",
    "                    \"/content/valid/76a48d818d14d2fc_jpg.rf.85ed567b1027bc95a6fe095882232ec8.jpg\",\n",
    "                    \"/content/valid/02b07e963a07d352_jpg.rf.299f6d50d4f9d7a83164433fc9471759.jpg\",\n",
    "                    \"/content/valid/9874930b8d717184_jpg.rf.0e93b01e011a2871ec3f47b8b7de04c7.jpg\",\n",
    "                    \"/content/valid/00383d8a0db75865_jpg.rf.2dec8c2bf4792306c26557485da36bad.jpg\",\n",
    "                    \"/content/valid/bb6502ebcd533137_jpg.rf.88dc373d5a9741a7a5f118581f0137aa.jpg\",\n",
    "                    \"/content/valid/9874930b8d717184_jpg.rf.0e93b01e011a2871ec3f47b8b7de04c7.jpg\",\n",
    "                    \"/content/valid/044eb325945ffb8c_jpg.rf.8ff6f75b4a0f906029cc4d34b38d5c36.jpg\",\n",
    "                    \"/content/valid/1c11f389a2f2673a_jpg.rf.231637a7a032bd38345e6804d9ed312f.jpg\",\n",
    "                    \"/content/valid/ca7a544fb7edb497_jpg.rf.87aebb71865dec3a490f92e02e2440e9.jpg\",\n",
    "                    \"/content/valid/9874930b8d717184_jpg.rf.0e93b01e011a2871ec3f47b8b7de04c7.jpg\",\n",
    "                    \"/content/valid/08b097deaa0fe7f5_jpg.rf.5a7d19e7281a5095f281d14fb5c1d037.jpg\",\n",
    "                    \"/content/valid/05d58de26ae6159a_jpg.rf.65b7e191f0c4432e3f5a450c9d7ad088.jpg\",\n",
    "                    \"/content/valid/76a48d818d14d2fc_jpg.rf.85ed567b1027bc95a6fe095882232ec8.jpg\",\n",
    "                    \"/content/valid/2ac96d4eed4e33c8_jpg.rf.0337788ab585230afe3350eedcd0c884.jpg\",\n",
    "                    \"/content/valid/2186b8a528ff9cb7_jpg.rf.dbd86730f6e1e7d3e1b758ca9ce71595.jpg\",\n",
    "                    \n",
    "]\n",
    "\n",
    "!mkdir -p /content/valid-30\n",
    "\n",
    "for img in TEST_IMAGE_PATHS:\n",
    "  !cp {img} /content/valid-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uIXoCCwkMjpK",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a213a2ae-9dc4-4637-987a-50249ef4d239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2021-11-30 11:59:07.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m240\u001b[0m - \u001b[1mArgs: Namespace(camid=0, ckpt='/content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/best_ckpt.pth.tar', conf=0.25, demo='image', device='gpu', exp_file='/content/YOLOX/exps/example/yolox_voc/yolox_voc_m.py', experiment_name='yolox_voc_m', fp16=False, fuse=False, name=None, nms=0.45, path='/content/valid-30', save_result=True, trt=False, tsize=640)\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001b[32m2021-11-30 11:59:07.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1mModel Summary: Params: 25.33M, Gflops: 73.76\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:10.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m261\u001b[0m - \u001b[1mloading checkpoint\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:10.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mloaded checkpoint done.\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:10.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.1286s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:10.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/00383d8a0db75865_jpg.rf.2dec8c2bf4792306c26557485da36bad.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:10.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.1149s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:10.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/016fbd083b98786f_jpg.rf.f9a56b569101cb481053060c3ba5e296.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.1114s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/02b07e963a07d352_jpg.rf.299f6d50d4f9d7a83164433fc9471759.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.1006s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/040659777cc851a1_jpg.rf.861a6d222877d7dede55a802885e7492.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0873s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/044eb325945ffb8c_jpg.rf.8ff6f75b4a0f906029cc4d34b38d5c36.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0877s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/05c680512a087536_jpg.rf.4da802147647e2b70427a54c417be2e0.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0871s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/05d58de26ae6159a_jpg.rf.65b7e191f0c4432e3f5a450c9d7ad088.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0845s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/08b097deaa0fe7f5_jpg.rf.5a7d19e7281a5095f281d14fb5c1d037.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0867s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:11.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/08e5905e139f6425_jpg.rf.d58a09d87843c60e9f1897a94fdc44a1.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0888s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/118cfaa4ead8036c_jpg.rf.ccf53544a798623be898ede526c24930.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0887s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/1c11f389a2f2673a_jpg.rf.231637a7a032bd38345e6804d9ed312f.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0872s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/2186b8a528ff9cb7_jpg.rf.dbd86730f6e1e7d3e1b758ca9ce71595.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0874s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/2ac96d4eed4e33c8_jpg.rf.0337788ab585230afe3350eedcd0c884.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0843s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/49f2147514665f24_jpg.rf.ce41e445f8c6276ae2bd0167c066cf5a.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0854s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/60628c08dffbb0ed_jpg.rf.32760210054dc74c2fd97967bc192a38.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0892s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/76a48d818d14d2fc_jpg.rf.85ed567b1027bc95a6fe095882232ec8.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0892s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:12.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/82af8ecf17d42a78_jpg.rf.778833c8052abbe3ee3f4e9e58870d63.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0856s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/94778d10168b17b0_jpg.rf.eaa84eb925565613161688f6487fc33c.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0878s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/9874930b8d717184_jpg.rf.0e93b01e011a2871ec3f47b8b7de04c7.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0879s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/bb6502ebcd533137_jpg.rf.88dc373d5a9741a7a5f118581f0137aa.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0872s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/ca7a544fb7edb497_jpg.rf.87aebb71865dec3a490f92e02e2440e9.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0845s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/ead5ec887d5c02a7_jpg.rf.b8c9fb402791dc4cf384476828bfd2d4.jpg\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mInfer time: 0.0870s\u001b[0m\n",
      "\u001b[32m2021-11-30 11:59:13.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m188\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_voc_m/vis_res/2021_11_30_11_59_10/fb81bce006d17076_jpg.rf.48415aa14ccfdd3497eb20783b5644d6.jpg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"/content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/best_ckpt.pth.tar\"\n",
    "FOLDER_PATH = \"/content/valid-30\"\n",
    "\n",
    "!python tools/demo.py image -f /content/YOLOX/exps/example/yolox_voc/yolox_voc_m.py -c {MODEL_PATH} --path {FOLDER_PATH} --conf 0.25 --nms 0.45 --tsize 640 --save_result --device gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7nX2nwWCper"
   },
   "source": [
    "# Visualize the Predictions\n",
    "Make sure you replace the `OUTPUT_IMAGE_PATH` with the respective path of the image output. This path can be found somewhere in the `YOLOX_outputs` folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "B3_kOK2cZtJK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "OUTPUT_IMAGE_FOLDER = \"/content/YOLOX/YOLOX_outputs/yolox_voc_s/vis_res/2021_09_14_02\"\n",
    "def get_full_path(img):\n",
    "  return os.path.join(OUTPUT_IMAGE_FOLDER, img)\n",
    "\n",
    "images = list(map(get_full_path, os.listdir(OUTPUT_IMAGE_FOLDER)))\n",
    "Image.open(images[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFbMKDkxPWoD"
   },
   "source": [
    "# Export Trained Weights for Future Inference\n",
    "\n",
    "Now that you have trained your custom detector, you can export the trained weights you have made here for inference on your device elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "SlZf3KlMPYPS",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "LDrqgjePPaXK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%cp {MODEL_PATH} /content/gdrive/My\\ Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQELubtjm0nc"
   },
   "source": [
    "# Download Experiment Outputs to Gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "D8joee_xm4ML",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "20165900-1362-44c8-9c0d-7924f6a1ac70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "jk4thRR1nSrj",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ab36c814-e8ed-4517-88a1-ed3327d2cc1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/ (stored 0%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/latest_ckpt.pth.tar (deflated 7%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/last_epoch_ckpt.pth.tar (deflated 7%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/val_log.txt (deflated 94%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/train_log.txt (deflated 90%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/best_ckpt.pth.tar (deflated 7%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/events.out.tfevents.1638252966.9ed57a06595e.905.0 (deflated 67%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/ (stored 0%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/ (stored 0%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/ca7a544fb7edb497_jpg.rf.87aebb71865dec3a490f92e02e2440e9.jpg (deflated 12%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/1c11f389a2f2673a_jpg.rf.231637a7a032bd38345e6804d9ed312f.jpg (deflated 8%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/60628c08dffbb0ed_jpg.rf.32760210054dc74c2fd97967bc192a38.jpg (deflated 5%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/bb6502ebcd533137_jpg.rf.88dc373d5a9741a7a5f118581f0137aa.jpg (deflated 7%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/118cfaa4ead8036c_jpg.rf.ccf53544a798623be898ede526c24930.jpg (deflated 12%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/94778d10168b17b0_jpg.rf.eaa84eb925565613161688f6487fc33c.jpg (deflated 5%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/9874930b8d717184_jpg.rf.0e93b01e011a2871ec3f47b8b7de04c7.jpg (deflated 7%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/ead5ec887d5c02a7_jpg.rf.b8c9fb402791dc4cf384476828bfd2d4.jpg (deflated 5%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/016fbd083b98786f_jpg.rf.f9a56b569101cb481053060c3ba5e296.jpg (deflated 5%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/49f2147514665f24_jpg.rf.ce41e445f8c6276ae2bd0167c066cf5a.jpg (deflated 4%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/08e5905e139f6425_jpg.rf.d58a09d87843c60e9f1897a94fdc44a1.jpg (deflated 4%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/05c680512a087536_jpg.rf.4da802147647e2b70427a54c417be2e0.jpg (deflated 6%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/2ac96d4eed4e33c8_jpg.rf.0337788ab585230afe3350eedcd0c884.jpg (deflated 5%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/08b097deaa0fe7f5_jpg.rf.5a7d19e7281a5095f281d14fb5c1d037.jpg (deflated 4%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/05d58de26ae6159a_jpg.rf.65b7e191f0c4432e3f5a450c9d7ad088.jpg (deflated 6%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/040659777cc851a1_jpg.rf.861a6d222877d7dede55a802885e7492.jpg (deflated 12%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/044eb325945ffb8c_jpg.rf.8ff6f75b4a0f906029cc4d34b38d5c36.jpg (deflated 4%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/00383d8a0db75865_jpg.rf.2dec8c2bf4792306c26557485da36bad.jpg (deflated 5%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/fb81bce006d17076_jpg.rf.48415aa14ccfdd3497eb20783b5644d6.jpg (deflated 4%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/02b07e963a07d352_jpg.rf.299f6d50d4f9d7a83164433fc9471759.jpg (deflated 4%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/2186b8a528ff9cb7_jpg.rf.dbd86730f6e1e7d3e1b758ca9ce71595.jpg (deflated 4%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/82af8ecf17d42a78_jpg.rf.778833c8052abbe3ee3f4e9e58870d63.jpg (deflated 7%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/vis_res/2021_11_30_11_59_10/76a48d818d14d2fc_jpg.rf.85ed567b1027bc95a6fe095882232ec8.jpg (deflated 5%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/events.out.tfevents.1638264440.9ed57a06595e.2538.0 (deflated 68%)\n",
      "  adding: content/YOLOX/YOLOX_outputs/variant-exp-yolox-m/last_mosaic_epoch_ckpt.pth.tar (deflated 7%)\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"variant-exp-yolox-m\"\n",
    "!cd /content/YOLOX/YOLOX_outputs\n",
    "!zip -r /content/{EXPERIMENT_NAME}.zip /content/YOLOX/YOLOX_outputs/{EXPERIMENT_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sLSTzh46n90h",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c87cb7cf-86e1-4a6f-ab17-c791a1107c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721M\t/content/variant-exp-yolox-m.zip\n"
     ]
    }
   ],
   "source": [
    "!du -h /content/{EXPERIMENT_NAME}.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "2LE4WaasoEB1",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!cp /content/{EXPERIMENT_NAME}.zip /content/gdrive/MyDrive/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "experiment-yolox-variants",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
